\documentclass[a4paper]{article}

\input{style/ch_xelatex.tex}
\input{style/scala.tex}

%代码段设置
\lstset{numbers=left,
basicstyle=\tiny,
numberstyle=\tiny,
keywordstyle=\color{blue!70},
commentstyle=\color{red!50!green!50!blue!50},
frame=single, rulesepcolor=\color{red!20!green!20!blue!20},
escapeinside=``
}

\graphicspath{ {figures/} }
\usepackage{ctex}
\setCJKmainfont[ItalicFont=Noto Sans CJK SC Bold, BoldFont=Noto Serif CJK SC Black]{Noto Serif CJK SC}
\usepackage{graphicx}
\usepackage{color,framed}%文本框
\usepackage{listings}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{bm} 
\usepackage{lastpage}%获得总页数
\usepackage{fancyhdr}
\usepackage{tabularx}  
\usepackage{geometry}
\usepackage{minted}
\usepackage{graphics}
\usepackage{subfigure}
\usepackage{float}
\usepackage{pdfpages}
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}
\usepackage{multirow}
\usepackage{footnote}
\usepackage{booktabs}

%-----------------------伪代码------------------
\usepackage{algorithm}  
\usepackage{algorithmicx}  
\usepackage{algpseudocode}  
\floatname{algorithm}{Algorithm}  
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  
\renewcommand{\algorithmicensure}{\textbf{Output:}} 
\usepackage{lipsum}  
\makeatletter
\newenvironment{breakablealgorithm}
  {% \begin{breakablealgorithm}
  \begin{center}
     \refstepcounter{algorithm}% New algorithm
     \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
     \renewcommand{\caption}[2][\relax]{% Make a new \caption
      {\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
      \ifx\relax##1\relax % #1 is \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
      \else % #1 is not \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
      \fi
      \kern2pt\hrule\kern2pt
     }
  }{% \end{breakablealgorithm}
     \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
  \end{center}
  }
\makeatother
%------------------------代码-------------------
\usepackage{xcolor} 
\usepackage{listings} 
\lstset{ 
breaklines,%自动换行
basicstyle=\small,
escapeinside=``,
keywordstyle=\color{ blue!70} \bfseries,
commentstyle=\color{red!50!green!50!blue!50},% 
stringstyle=\ttfamily,% 
extendedchars=false,% 
linewidth=\textwidth,% 
numbers=left,% 
numberstyle=\tiny \color{blue!50},% 
frame=trbl% 
rulesepcolor= \color{ red!20!green!20!blue!20} 
}

%-------------------------页面边距--------------
\geometry{a4paper,left=2.3cm,right=2.3cm,top=2.7cm,bottom=2.7cm}
%-------------------------页眉页脚--------------
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\kaishu \leftmark}
% \chead{}
\rhead{\kaishu 并行程序设计实验报告}%加粗\bfseries 
\lfoot{}
\cfoot{\thepage}
\rfoot{}
\renewcommand{\headrulewidth}{0.1pt}  
\renewcommand{\footrulewidth}{0pt}%去掉横线
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}%标题横线
\newcommand{\HRulegrossa}{\rule{\linewidth}{1.2mm}}
\setlength{\textfloatsep}{10mm}%设置图片的前后间距
%--------------------文档内容--------------------

\begin{document}
\renewcommand{\contentsname}{目\ 录}
\renewcommand{\appendixname}{附录}
\renewcommand{\appendixpagename}{附录}
\renewcommand{\refname}{参考文献}
\renewcommand{\figurename}{图}
\renewcommand{\tablename}{表}
\renewcommand{\today}{\number\year 年 \number\month 月 \number\day 日}

%-------------------------封面----------------
\begin{titlepage}
  \begin{center}
    \includegraphics[width=0.8\textwidth]{NKU.png}\\[1cm]
    \vspace{20mm}
    \textbf{\huge\textbf{\kaishu{计算机学院}}}\\[0.5cm]
    \textbf{\huge{\kaishu{并行程序设计第 7 次作业}}}\\[2.3cm]
    \textbf{\Huge\textbf{\kaishu{高斯消去法的 MPI 并行化}}}

    \vspace{\fill}

    % \textbf{\Large \textbf{并行程序设计期末实验报告}}\\[0.8cm]
    % \HRule \\[0.9cm]
    % \HRule \\[2.0cm]
    \centering
    \textsc{\LARGE \kaishu{姓名\ :\ 丁屹}}\\[0.5cm]
    \textsc{\LARGE \kaishu{学号\ :\ 2013280}}\\[0.5cm]
    \textsc{\LARGE \kaishu{专业\ :\ 计算机科学与技术}}\\[0.5cm]

    \vfill
    {\Large \today}
  \end{center}
\end{titlepage}

\renewcommand {\thefigure}{\thesection{}.\arabic{figure}}%图片按章标号
\renewcommand{\figurename}{图}
\renewcommand{\contentsname}{目录}
\cfoot{\thepage\ of \pageref{LastPage}}%当前页 of 总页数


% 生成目录
\clearpage
\tableofcontents
\newpage

\section{问题描述}
高斯消去的计算模式如图 \ref{gauss} 所示，在第$k$步时，对第$k$行从$(k, k)$开始进行除法操作，并且将后续的$k + 1$至$N$行进行减去第$k$行的操作，串行算法如下面伪代码所示。
\begin{figure}
  \centering
  \includegraphics[width=1\textwidth]{gauss.png}
  \caption{高斯消去法示意图}
  \label{gauss}
\end{figure}
\begin{breakablealgorithm}
  \caption{普通高斯消元算法伪代码}
  \begin{algorithmic}[1] %每行显示行号  
    \Function {LU}{}
    \For {$k:=0$\ \textbf{to}\ $n$}
    \For {$j:=k+1$\ \textbf{to}\ $n$}
    \State {$A[k,j]:=A[k,j]/A[k,k]$}
    \EndFor
    \State{$A[k,k]:=1.0$}
    \For {$i:=k+1$\ \textbf{to}\ $n$}
    \For {$j:=k+1$\ \textbf{to}\ $n$}
    \State {$A[i,j]:=A[i,j]-A[i,k]*A[k,j]$}
    \EndFor
    \State{$A[i,k]:=0$}
    \EndFor
    \EndFor
    \EndFunction
  \end{algorithmic}
\end{breakablealgorithm}

观察高斯消去算法，注意到伪代码第 4，5 行第一个内嵌循环中的$A[k, j] := A[k, j]/A[k, k]$以及伪代码第$8，9，10$行双层$for$循环中的$A[i, j] := A[i, j] − A[i, k]×A[k, j]$都是可以进行向量化的循环。可以通过 MPI 对这两步进行并行优化。

\section{算法设计}

源码链接：
\url{https://github.com/ArcanusNEO/Parallel-Programming/tree/master/7}

\subsection{测试用例的确定}

由于测试数据集较大，不便于各个平台同步，所以采用固定随机数种子为 $12345687$ 的 mt19937随机数生成器。
经过实验发现不同规模下，所有元素独立生成，限制大小在 $[0, 100]$，能够生成可以被正确消元的矩阵。

代码如下：

\begin{lstlisting}[frame=trbl,language={C++}]
  uniform_real_distribution<float> dist(0, 100);
  mt19937       mt(12345687);
  int           n;
  istringstream iss(argv[1]);
  iss >> n;
  cout << n << endl;
  for (int i = 1; i <= n; ++i)
    for (int j = 1; j <= n; ++j) cout << dist(mt) << " \n"[j == n];
\end{lstlisting}

\subsection{实验环境和相关配置}

实验在本地 x86 Arch Linux 环境下完成，使用 cmake 构建项目，开启 O3 加速；

MPI库采用 OpenMPI，由于使用的 CPU 为 8C16T，所以足以应付最多 4 核心，每个进程 4 线程的负载。

\subsection{默认平凡算法}

使用一维数组模拟矩阵，避免改变矩阵大小时第二维不方便调整、必须设成最大值的问题，可以减少 cache 失效；

使用 $\#define\ matrix(i, j)\ arr[(i) * n + (j)]$ 宏，增强可读性；

\begin{lstlisting}[frame=trbl,language={C++}]
void func(int& ans, float arr[], int n) {
  for (int k = 0; k < n; ++k) {
    for (int j = k + 1; j < n; ++j) matrix(k, j) = matrix(k, j) / matrix(k, k);
    matrix(k, k) = 1.0;
    for (int i = k + 1; i < n; ++i) {
      for (int j = k + 1; j < n; ++j)
        matrix(i, j) = matrix(i, j) - matrix(i, k) * matrix(k, j);
      matrix(i, k) = 0;
    }
  }
}
\end{lstlisting}

\subsection{MPI 并行化算法}
采用了按照行划分的分割方法，每个进程分配到固定的行，某行除法工作由负责的进程计算，计算完成后全局同步。如果矩阵的秩不能被进程数整除，则将余数行划分给最后一个进程；如果矩阵的秩小于进程数，则最后一个进程会承担所有工作，因此需要避免这种情况。
\begin{lstlisting}[frame=trbl,language={C++}]
void func(int& ans, float arr[], int n) {
  int comm_sz;
  int my_rank;
  MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);
  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);

  MPI_Bcast(arr, n * n, MPI_FLOAT, 0, MPI_COMM_WORLD);

  int block_sz  = n / comm_sz;
  int row_begin = block_sz * my_rank;
  int row_end   = (my_rank + 1 == comm_sz ? n : row_begin + block_sz);

  for (int k = 0; k < n; ++k) {
    if (row_begin <= k && k < row_end) {
      for (int j = k + 1; j < n; ++j) matrix(k, j) /= matrix(k, k);
      matrix(k, k) = 1.0;
    }
    int bc_rank = comm_sz - 1;
    if (block_sz && k / block_sz < bc_rank) bc_rank = k / block_sz;
    MPI_Bcast(prow(k), n, MPI_FLOAT, bc_rank, MPI_COMM_WORLD);
    for (int i = max(row_begin, k + 1); i < row_end; ++i) {
      for (int j = k + 1; j < n; ++j)
        matrix(i, j) -= matrix(i, k) * matrix(k, j);
      matrix(i, k) = 0;
    }
  }
}
\end{lstlisting}

\subsection{MPI 并行化算法结合 OpenMP 多线程库}
MPI是非共享内存的多进程模型，OpenMP是共享内存的多线程模型，可以混合使用，在除法和消去阶段都可以进一步划分循环：除法部分按列划分，消去部分动态按行划分。需要注意的是，在MPI通信过程中只能单线程调用MPI\_Bcast函数，在通信前需要设置barrier保证所有线程都完成了除法阶段，在通信后也需要设置barrier保证所有线程等待同步行结束再执行消去。
\begin{lstlisting}[frame=trbl,language={C++}]
#define THREADS 4

#define matrix(i, j)  (arr[(i) * (n) + (j)])
#define pmatrix(i, j) (arr + ((i) * (n) + (j)))
#define prow(i)       (pmatrix(i, 0))

void func(int& ans, float arr[], int n) {
  int comm_sz;
  int my_rank;
  MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);
  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);

  MPI_Bcast(arr, n * n, MPI_FLOAT, 0, MPI_COMM_WORLD);

  int block_sz  = n / comm_sz;
  int row_begin = block_sz * my_rank;
  int row_end   = (my_rank + 1 == comm_sz ? n : row_begin + block_sz);

  int   i, j, k;
  int   bc_rank;
  float tmp;
#pragma omp parallel num_threads(THREADS), private(i, j, k, tmp, bc_rank)
  for (k = 0; k < n; ++k) {
    if (row_begin <= k && k < row_end) {
      tmp = matrix(k, k);
#pragma omp for
      for (j = k + 1; j < n; ++j) matrix(k, j) /= tmp;
      matrix(k, k) = 1.0;
    }
    bc_rank = comm_sz - 1;
    if (block_sz && k / block_sz < bc_rank) bc_rank = k / block_sz;
#pragma omp barrier
    if (omp_get_thread_num() == 0)
      MPI_Bcast(prow(k), n, MPI_FLOAT, bc_rank, MPI_COMM_WORLD);
#pragma omp barrier
#pragma omp for
    for (i = max(row_begin, k + 1); i < row_end; ++i) {
      tmp = matrix(i, k);
      for (j = k + 1; j < n; ++j) matrix(i, j) -= tmp * matrix(k, j);
      matrix(i, k) = 0;
    }
  }
}
\end{lstlisting}

\subsection{MPI 并行化算法结合 SIMD 指令优化}
NEON或AVX优化作用于循环的CPU核心内并行化，由于O3级别的优化可能已经引入了SIMD，效果不一定明显。
\begin{lstlisting}[title=AVX优化,frame=trbl,language={C++}]
#define matrix(i, j)  (arr[(i) * (n) + (j)])
#define pmatrix(i, j) (arr + ((i) * (n) + (j)))
#define prow(i)       (pmatrix(i, 0))

void func(int& ans, float arr[], int n) {
  int comm_sz;
  int my_rank;
  MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);
  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);

  MPI_Bcast(arr, n * n, MPI_FLOAT, 0, MPI_COMM_WORLD);

  int block_sz  = n / comm_sz;
  int row_begin = block_sz * my_rank;
  int row_end   = (my_rank + 1 == comm_sz ? n : row_begin + block_sz);

  for (int k = 0; k < n; ++k) {
    int j;
    if (row_begin <= k && k < row_end) {
      auto vt =
        _mm256_set_ps(matrix(k, k), matrix(k, k), matrix(k, k), matrix(k, k),
                      matrix(k, k), matrix(k, k), matrix(k, k), matrix(k, k));
      for (j = k + 1; j + 8 <= n; j += 8) {
        auto va = _mm256_loadu_ps(pmatrix(k, j));
        va      = _mm256_div_ps(va, vt);
        _mm256_storeu_ps(pmatrix(k, j), va);
      }
      for (; j < n; ++j) matrix(k, j) /= matrix(k, k);
      matrix(k, k) = 1.0;
    }
    int bc_rank = comm_sz - 1;
    if (block_sz && k / block_sz < bc_rank) bc_rank = k / block_sz;
    MPI_Bcast(prow(k), n, MPI_FLOAT, bc_rank, MPI_COMM_WORLD);
    for (int i = max(row_begin, k + 1); i < row_end; ++i) {
      auto vaik =
        _mm256_set_ps(matrix(i, k), matrix(i, k), matrix(i, k), matrix(i, k),
                      matrix(i, k), matrix(i, k), matrix(i, k), matrix(i, k));
      for (j = k + 1; j + 8 <= n; j += 8) {
        auto vakj = _mm256_loadu_ps(pmatrix(k, j));
        auto vaij = _mm256_loadu_ps(pmatrix(i, j));
        auto vx   = _mm256_mul_ps(vakj, vaik);
        vaij      = _mm256_sub_ps(vaij, vx);
        _mm256_storeu_ps(pmatrix(i, j), vaij);
      }
      for (; j < n; ++j) matrix(i, j) -= matrix(i, k) * matrix(k, j);
      matrix(i, k) = 0;
    }
  }
}
\end{lstlisting}

\begin{lstlisting}[title=NEON优化,frame=trbl,language={C++}]
#define matrix(i, j)  (arr[(i) * (n) + (j)])
#define pmatrix(i, j) (arr + ((i) * (n) + (j)))
#define prow(i)       (pmatrix(i, 0))

void func(int& ans, float arr[], int n) {
  int comm_sz;
  int my_rank;
  MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);
  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);

  MPI_Bcast(arr, n * n, MPI_FLOAT, 0, MPI_COMM_WORLD);

  int block_sz  = n / comm_sz;
  int row_begin = block_sz * my_rank;
  int row_end   = (my_rank + 1 == comm_sz ? n : row_begin + block_sz);

  for (int k = 0; k < n; ++k) {
    int j;
    if (row_begin <= k && k < row_end) {
      auto vt = vdupq_n_f32(matrix(k, k));
      for (j = k + 1; j + 4 <= n; j += 4) {
        auto va = vld1q_f32(pmatrix(k, j));
        va      = vdivq_f32(va, vt);
        vst1q_f32(pmatrix(k, j), va);
      }
      for (; j < n; ++j) matrix(k, j) = matrix(k, j) / matrix(k, k);
      matrix(k, k) = 1.0;
    }
    int bc_rank = comm_sz - 1;
    if (block_sz && k / block_sz < bc_rank) bc_rank = k / block_sz;
    MPI_Bcast(prow(k), n, MPI_FLOAT, bc_rank, MPI_COMM_WORLD);
    for (int i = max(row_begin, k + 1); i < row_end; ++i) {
      auto vaik = vdupq_n_f32(matrix(i, k));
      for (j = k + 1; j + 4 <= n; j += 4) {
        auto vakj = vld1q_f32(pmatrix(k, j));
        auto vaij = vld1q_f32(pmatrix(i, j));
        auto vx   = vmulq_f32(vakj, vaik);
        vaij      = vsubq_f32(vaij, vx);
        vst1q_f32(pmatrix(i, j), vaij);
      }
      for (; j < n; ++j) matrix(i, j) -= matrix(i, k) * matrix(k, j);
      matrix(i, k) = 0;
    }
  }
}
\end{lstlisting}

\section{算法分析}
\subsection{时间复杂度分析}
按行划分的MPI算法分成$k$个步骤，第$k$行的除法部分计算次数为$n-k-1$，设MPI节点数为$N$，则并行消去部分的计算时间为$\frac{(n-k-1) \cdot n}{N}$，则计算部分的时间复杂度为$\sum_{k=0}^{n-1}\frac{(n-k-1) \cdot n}{N} \sim O(\frac{n^3}{N})$。由于使用了MPI\_Bcast通信，单次通信时间复杂度为$O(n\log{N})$，则总通信时间复杂度为$\sum_{k=0}^{n-1}n\log{N} \sim O(n^2\log{N})$，所以总时间复杂度为$O(\frac{n^3}{N})$

加入OpenMP的算法计算部分时间复杂度可以继续除以线程数$T$，得到时间复杂度$O(\frac{n^3}{N \cdot T})$；对于 NEON 优化的算法，类似的除以向量长度常数。

\subsection{运行时间分析}
由于本人在华为鲲鹏服务器上的帐号出现异常，获取不到pbs系统的标准输出和标准错误重定向文件，即便显示指定输出路径也无效，所以只在本地 x86 环境实验。

\begin{table}[]
  \centering
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{|ll|llll|}
      \hline
      Scale       & Reperat times & x86 ordinary (s) & x86 MPI 2 (s)  & x86 MPI 4 (s)  & x86 MPI 8 (s)  \\ \hline
      8×8         & 100           & 0.000001330460   & 0.000003109250 & 0.000007227200 & 0.000011003490 \\ \hline
      16 × 16     & 50            & 0.000001706920   & 0.000004909840 & 0.000011357620 & 0.000018678300 \\ \hline
      32 × 32     & 50            & 0.000003640080   & 0.000009848960 & 0.000024183300 & 0.000037996420 \\ \hline
      64 × 64     & 20            & 0.000015253300   & 0.000022858900 & 0.000050830400 & 0.000078725100 \\ \hline
      128 × 128   & 15            & 0.000098880800   & 0.000099426133 & 0.000160350867 & 0.000224362867 \\ \hline
      256 × 256   & 10            & 0.000716408500   & 0.000450790200 & 0.000536723300 & 0.000705872000 \\ \hline
      512 × 512   & 10            & 0.006722607300   & 0.003132060100 & 0.002446903000 & 0.003516368600 \\ \hline
      1024 × 1024 & 5             & 0.064893815400   & 0.032939996400 & 0.021467574400 & 0.017404530400 \\ \hline
      2048 × 2048 & 3             & 1.400074583333   & 1.033122249667 & 0.851402086333 & 0.774419514333 \\ \hline
      4096 × 4096 & 1             & 10.705585484000  & 8.511468985000 & 8.007275085000 & 7.703194050000 \\ \hline
    \end{tabular}%
  }
  \caption{平凡算法和不同核心数 MPI 加速算法运行时间}
  \label{tab:ord-mpi}
\end{table}

\begin{table}[]
  \centering
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{|ll|llll|}
      \hline
      Scale       & Reperat times & x86 ordinary (s) & x86 MPI AVX 2 (s) & x86 MPI AVX 4 (s) & x86 MPI AVX 8 (s) \\ \hline
      8×8         & 100           & 0.000001330460   & 0.000002948670    & 0.000007216750    & 0.000011136140    \\ \hline
      16 × 16     & 50            & 0.000001706920   & 0.000004758920    & 0.000012403820    & 0.000019389440    \\ \hline
      32 × 32     & 50            & 0.000003640080   & 0.000009745640    & 0.000025260240    & 0.000038144480    \\ \hline
      64 × 64     & 20            & 0.000015253300   & 0.000023498100    & 0.000051647500    & 0.000079584050    \\ \hline
      128 × 128   & 15            & 0.000098880800   & 0.000097968467    & 0.000165770600    & 0.000224795867    \\ \hline
      256 × 256   & 10            & 0.000716408500   & 0.000441564300    & 0.000561866000    & 0.000702323900    \\ \hline
      512 × 512   & 10            & 0.006722607300   & 0.003193946400    & 0.002559661900    & 0.003394705300    \\ \hline
      1024 × 1024 & 5             & 0.064893815400   & 0.027655931200    & 0.020635136600    & 0.017636375600    \\ \hline
      2048 × 2048 & 3             & 1.400074583333   & 1.340856775000    & 0.865629919333    & 0.759849924667    \\ \hline
      4096 × 4096 & 1             & 10.705585484000  & 11.593806475000   & 8.896995985000    & 8.144267148000    \\ \hline
    \end{tabular}%
  }
  \caption{平凡算法和不同核心数 MPI 及 AVX 加速算法运行时间}
  \label{tab:ord-mpi-avx}
\end{table}

\begin{table}[]
  \centering
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{|ll|ll|}
      \hline
      Scale       & Reperat times & x86 ordinary (s) & x86 MPI OpenMP (s) \\ \hline
      8×8         & 100           & 0.000001330460   & 0.000016614600     \\ \hline
      16 × 16     & 50            & 0.000001706920   & 0.000031428600     \\ \hline
      32 × 32     & 50            & 0.000003640080   & 0.000073612800     \\ \hline
      64 × 64     & 20            & 0.000015253300   & 0.000124789050     \\ \hline
      128 × 128   & 15            & 0.000098880800   & 0.000303953933     \\ \hline
      256 × 256   & 10            & 0.000716408500   & 0.000936229300     \\ \hline
      512 × 512   & 10            & 0.006722607300   & 0.002821468900     \\ \hline
      1024 × 1024 & 5             & 0.064893815400   & 0.015939609600     \\ \hline
      2048 × 2048 & 3             & 1.400074583333   & 0.676996145333     \\ \hline
      4096 × 4096 & 1             & 10.705585484000  & 9.719432846000     \\ \hline
    \end{tabular}%
  }
  \caption{平凡算法和 MPI 及 OpenMP 加速算法运行时间}
  \label{tab:ord-mpi-omp}
\end{table}

其中单纯的MPI算法测试了 2、4、8 节点时的运行时间（表\ref{tab:ord-mpi}），矩阵越小重复测试数量越多。MPI加上AVX加速也测试了 2、4、8 节点时的运行时间（表\ref{tab:ord-mpi-avx}）。对于 MPI 加上 OpenMP 加速算法，由于本地环境 CPU 最多 16 线程，所以只测试了 4 节点，每节点 4 线程的运行时间（表\ref{tab:ord-mpi-omp}）。

可以发现，不同加速策略均有一定程度的加速效果，在较大的矩阵中加速效果大于 0\%，且遵循并行度越高加速比越大的规律。但是可能是由于本地 CPU 环境不适用使用 MPI 非共享内存的多进程模型，所以加速效果不明显：相对 OpenMP 多线程模型而言，MPI 的通信开销非常大。

% \newpage

% \section{参考文献}
% \cite{1}\cite{2}\cite{3}\cite{4}\cite{5}\cite{6}

% \bibliographystyle{plain}
% \bibliography{Parallel-Programming-0.bib}

\end{document}